%%=============================================================================
%% Geteste modellen
%%=============================================================================

\chapter{Werkwijze en resultaten}%
\label{ch:resultaten}
In dit hoofdstuk wordt besproken wat gedaan werd tijdens dit onderzoek, wat de resultaten waren en waarom bepaalde keuzes gemaakt werden.

\section{Geteste modellen}
\label{sec:modellen}
Na het opzoeken van bestaande opties werd gekozen om 3 verschillende modellen uit te testen. Het eerste model dat getest werd, is Speaker-Diarization van \textcite{DongLu}. Dit is een model dat gebaseerd is op VGG-Speaker-recognition en UIS-RNN. Na wat extra onderzoek bleek echter dat de combinatie van beide frameworks en het trainen zelf nog van nul zouden moeten gebeuren, wat veel tijd in beslag zou nemen, dus werd er gekozen om hier voor deze bachelorproef geen gebruik van te maken.

Het tweede model dat getest werd, is Simple Diarizer van \textcite{Chau}. Dit leek op het eerste zicht een goed model, maar na tijdens het testen bleek dat dit model gebruik maakt van SpeechBrain. SpeechBrain is een ``all-in-one speech toolkit" die ontwikkeld is om onderzoek naar en het ontwikkelen van neurale spraakverwerkingstechnologieën gemakkelijker te maken \autocite{speechbrain}. Het gebruik hiervan bracht echter het probleem met zich mee dat admin rechten nodig waren om er gebruik van te kunnen maken, dus werd er besloten om toch niet met dit model verder te gaan.

Het laatste model dat getest werd was pyannote-audio van \textcite{Bredin2024}. Dit model werd getest op een Vlaams audio fragment en gaf als resultaat een de start- en eindtijden voor elke spreker weer. Er is echter geen optie voorzien om dit om te zetten naar audio-fragmenten. Dit probleem was eenvoudig op te lossen door een externe python library te gebruiken, namelijk pydub. Na het beluisteren van de audiofragmenten bleek dat deze nog niet zeer accuraat waren en dat er dus nog training nodig was, maar het resultaat was al een zeer goed begin. Aangezien pyannote-audio voorzien is om nog verder getraind te worden, werd er gekozen om met dit model verder te gaan. De resultaten hiervan worden in de volgende secties besproken.

\section{Voorbereiden van de data}
\label{sec:data}
Om een pyannote.audio model te trainen is een pyannote databank nodig. Hoe je deze juist kan opstellen staat beschreven op \textcite{BredinDatabase} voor verschillende types modellen. Hier wordt specifiek gebruik gemaakt van de set-up voor een het trainen van een speaker diarization model.

De basis van de databank is een YAML bestand waarin een databank en een protocol staan. De databank verwijst naar de plaats waar de audio bestanden opgeslagen staan in de vorm van wav bestanden. Het protocol bestaat uit 3 subsets, namelijk een train subset die gebruikt wordt om het model te trainen, een development subset die gebruikt wordt als validatie en een test subset die gebruikt wordt voor de evaluatie van het model. Elk van deze subsets verwijzen naar een .rttm, .uem en .lst bestand. Het .rttm bestand bevat de annotatie van de audiofragmenten die gebruikt worden in de subset, het .uem bestand bevat de informatie over welke delen van de audio bestanden geannoteerd zijn voor de subset en het .lst bestand bevat een lijst met de namen van bestanden die gebruikt worden in de subset. De uiteindelijke set-up van het YAML bestand is te zien in codefragment ~\ref{code:config-file}.

\begin{listing}
    \begin{minted}{yaml}
Databases:
  BP: 
    - ../training_database/wavs/{uri}.wav

Protocols:
  BP:
    SpeakerDiarization:
      VlaamseAudio:
        scope: file
        train:
          uri: ../training_database/lists/train.lst
          annotation: ../training_database/rttms/train.rttm
          annotated: ../training_database/uems/train.uem
        development:
          uri: ../training_database/lists/development.lst
          annotation: ../training_database/rttms/development.rttm
          annotated: ../training_database/uems/development.uem
        test:
          uri: ../training_database/lists/test.lst
          annotation: ../training_database/rttms/test.rttm
          annotated: ../training_database/uems/test.uem
    \end{minted}
	\caption[Configuratie databank]{\label{code:config-file}Inhoud van het configuratiebestand voor de databank}
\end{listing}

Tijdens het verloop van het trainen waren er drie versies van de databank. De eerste versie bestond enkel uit audio van televisieprogramma's. In de tweede versie werd er data toegevoegd die gedurende voorgaande jaren opgenomen werd in het Zorglab. In de derde versie werd een herverdeling gedaan van de data uit de eerste versie.

Voor de keuze van de gebruikte audio bestanden, waren er een aantal zaken waarmee rekening gehouden werd. Het voornaamste waarnaar gekeken werd, was de duur van het audio fragment. Er werd gekozen om vooral gebruik te maken van audio fragmenten die niet langer waren dan 5 tot 10 minuten, omdat langere bestanden meer tijd en resources nodig zouden hebben om te verwerken. Een andere factor die meespeelde bij het zoeken naar audio fragmenten was de hoeveelheid overlap er was tussen de stemmen. Er werd gekozen om vooral fragmenten te gebruiken met minder overlap, omdat dit anders voor een te hoge complexiteit van de bestanden zou zorgen.

Wanneer het komt op de verdeling van de data over de train, test en validatie bestanden, werden er enkel rekening gehouden met de hoeveelheid data die voor elke set gebruikt zou worden. \textcite{BredinDatabase} raadt aan om een verdeling van 0.8-0.1-0.1 toe te passen. Dit houdt in dat 80\% van de data gebruikt wordt voor het trainen, 10\% voor het testen en 10\% voor de validatie.

Bij de eerste versie van de databank werden niet alle audio bestanden die gevonden werden gebruikt in het opstellen van de databank, omdat het oorspronkelijke doel was meer data toe te voegen na een eerste test, maar uiteindelijk gekozen werd om data uit de audio van het Zorglab te gebruiken. Er werd gebruik gemaakt van fragmenten uit 10 audio bestanden en de .rttm files omvatten een totaal van 168 lijnen. In tabel \ref{tbl:verdeling-v1} wordt de verdeling van deze data over de train, test en validatie bestanden voorgesteld.

\begin{table}[]
    \begin{tabular}{@{}lcc@{}}
        \toprule
                             & \multicolumn{1}{l}{\textbf{Aantal bestanden}} & \multicolumn{1}{l}{\textbf{Aantal lijnen}} \\ \midrule
        \textbf{Train}       & 7                                             & 129                                        \\
        \textbf{Test}        & 2                                             & 16                                         \\
        \textbf{Development} & 1                                             & 23                                         \\ \bottomrule
    \end{tabular}
    \caption[Verdeling data eerste versie databank]{\label{tbl:verdeling-v1}Verdeling van de data over de train, test en validatie bestanden in de eerste versie van de databank}
\end{table}

Voor de tweede versie van de databank werd de data uit de audiobestanden van het Zorglab toegevoegd aan de eerste versie van de databank. Hiervoor werd gebruik gemaakt van fragmenten uit 7 bestanden wat leidde tot een totaal van 394 lijnen in de .rttm files. In tabel \ref{tbl:verdeling-v2} wordt de verdeling van deze data over de train, test en validatie bestanden voorgesteld. Ook hier werd voor de verdeling van de data over de train, test en validatie bestanden niet met specifieke criteria rekening gehouden, behalve dat elke subset minstens een bestand van het Zorglab bevat.

\begin{table}[]
    \begin{tabular}{@{}lcc@{}}
        \toprule
        & \multicolumn{1}{l}{\textbf{Aantal bestanden}} & \multicolumn{1}{l}{\textbf{Aantal lijnen}} \\ \midrule
        \textbf{Train}       & 12                                            & 316                                        \\
        \textbf{Test}        & 2                                             & 37                                         \\
        \textbf{Development} & 2                                             & 41                                         \\ \bottomrule
    \end{tabular}
    \caption[Verdeling data tweede versie databank]{\label{tbl:verdeling-v2}Verdeling van de data over de train, test en validatie bestanden in de tweede versie van de databank}
\end{table}

Voor de derde versie van de databank werd teruggegrepen naar de data uit de eerste versie van de databank, met als verschil dat er een herverdeling van de data plaatsvond over de train, test en validatie subsets. Bij deze herverdeling werd ervoor gezorgd dat de verdeling van het aantal lijnen in de .rttm bestanden iets dichter ligt bij de 0.8-0.1-0.1 verdeling en dat er voor zowel de test als de validatie subset gebruik gemaakt wordt van de data van 2 audio bestanden. De verdeling van deze versie van de databank wordt voorgesteld in tabel \ref{tbl:verdeling-v3}.

\begin{table}[]
    \begin{tabular}{@{}lcc@{}}
        \toprule
        & \multicolumn{1}{l}{\textbf{Aantal bestanden}} & \multicolumn{1}{l}{\textbf{Aantal lijnen}} \\ \midrule
        \textbf{Train}       & 6                                             & 132                                        \\
        \textbf{Test}        & 2                                             & 17                                         \\
        \textbf{Development} & 2                                             & 19                                         \\ \bottomrule
    \end{tabular}
    \caption[Verdeling data tweede versie databank]{\label{tbl:verdeling-v3}Verdeling van de data over de train, test en validatie bestanden in de derde versie van de databank}
\end{table}

\section{Trainen en finetunen van het gekozen model}
\label{sec:trainen}

\subsection{Trainingsproces}
\label{subsec:proces}
Het proces om het model te trainen en finetunen werd gebaseerd op een voorbeeld notebook \footnote{Voorbeeld notebook voor het trainingsproces: \url{https://github.com/pyannote/pyannote-audio/blob/main/tutorials/adapting_pretrained_pipeline.ipynb}} die te vinden is op de GitHub pagina van het model \autocite{Bredin2024}. Voor het trainen werd gebruik gemaakt WSL (Windows Subsystem for Linux) omdat dit niet mogelijk is in Windows.

De eerste stap in het trainingsproces is het inladen van de databank in de notebook. Hierna wordt de pyannote-audio pipeline voor speaker diarization opgehaald. Hiervoor zijn 2 methoden:
\begin{itemize}
	\item De pipeline die op Hugging Face staat ophalen \footnote{Pyannote speaker diarization pipeline op Hugging Face: \url{https://huggingface.co/pyannote/speaker-diarization-3.1}}
	\item Gebruik maken van de offline pipeline
\end{itemize}
Hier werd gekozen voor het gebruik maken van de offline pipeline, aangezien deze na het trainen opgeslagen moest kunnen worden voor implementatie in de applicatie, wat moeilijker was met de online pipeline.

Na het inladen van de databank en het ophalen van de pipeline wordt de accuraatheid van de pipeline berekend door gebruik te maken van pyannote-metrics. Hiervoor creëert de pipeline eerst een RTTM bestand van de audio bestanden in de test subset van de databank. Hierna wordt dit vergeleken met de RTTM bestanden in de test subset en op basis daarvan wordt de DER berekend. De code die hiervoor gebruikt wordt, is te zien in codefragment \ref{code:pyannote-metrics}.

\begin{listing}
\begin{minted}{python}
from pyannote.metrics.diarization import DiarizationErrorRate

metric = DiarizationErrorRate()

for file in protocol.test():
  print(file)
  file["pretrained pipeline"] = pretrained_pipeline(file)
  metric(file["annotation"], file["pretrained pipeline"], uem=file["annotated"])

print(f"Diarization error rate is {100 * abs(metric):.1f}% for the pretrained model")
\end{minted}
	\caption[Berekening DER met pyannote-metrics]{\label{code:pyannote-metrics}Berekening van de accuraatheid van de speaker diarization pipeline}
\end{listing}

Voor aan het trainen begonnen kan worden, moet segmentatie model uit de pipeline gehaald worden, aangezien de training hierop zal gebeuren. Aan dit model wordt eerst een taak aan toegewezen, waarna de data preparatie en set-up gebeuren zoals getoond in codefragment \ref{code:setup-training}.

\begin{listing}
	\begin{minted}{python}
pretrained_segm_model = Model.from_pretrained("./pipeline/segmentation/pytorch_model.bin")
output_dir = "./models"

task = pyannote.audio.tasks.SpeakerDiarization(
  protocol,
  max_num_speakers=5,
  batch_size=32,
  vad_loss="bce"
)

pretrained_segm_model.task = task
pretrained_segm_model.prepare_data()
pretrained_segm_model.set-up()
	\end{minted}
	\caption[Model klaarmaken voor training]{\label{code:setup-training}Code die gebruikt wordt om het model klaar te maken voor training.}
\end{listing}

Het trainen zelf gebeurt door gebruik te maken van PyTorch Lightning \footnote{Documentatie voor PyTorch Lightning; \url{https://lightning.ai/docs/pytorch/stable/}}. Hiervoor wordt gebruik gemaakt van een Adam optimizer en een checkpoint voor EarlyStopping. De set-up hiervoor wordt getoond in codefragment \ref{code:pl-setup}.

\begin{listing}
	\begin{minted}{python}
def configure_optimizers(self):
  return Adam(self.parameters(), lr=1e-4)
  
pretrained_segm_model.configure_optimizers = MethodType(configure_optimizers, pretrained_segm_model)

monitor, direction = task.val_monitor
checkpoint = ModelCheckpoint(
  monitor,
  mode=direction,
  save_top_k=1,
  every_n_epochs=1,
  save_last=False,
  save_weights_only=False,
  filename="{epoch}",
  verbose=False
)

early_stopping = EarlyStopping(
  monitor=monitor,
  mode=direction,
  min_delta=0.0,
  patience=10,
  strict=True,
  verbose=False
)

callbacks = [RichProgressBar(), checkpoint, early_stopping]

trainer = pl.Trainer(
  callbacks=callbacks,
  max_epochs=20,
  gradient_clip_val=0.5,
  log_every_n_steps=5
)
	\end{minted}
	\caption[PyTorch Lightning set-up]{\label{code:pl-setup}Opzetten van de PyTorch Lightning trainer}
\end{listing}

Na deze stappen kan de effectieve training gebeuren en wordt het beste model uit de checkpoints opgeslagen.

Na het trainen wordt de pipeline stap voor stap opnieuw opgebouwd en gefinetuned. Tijdens de eerste stap wordt een pipeline gecreëerd met enkel het getrainde segmentatie model en OracleClustering als clustering methode. Als parameters wordt enkel gebruik gemaakt van "min\_duration\_off`` op 0.0 seconden voor het segmentatie model, wat ervoor zorgt dat gebieden zonder spraak die korter zijn dan 0.0 seconden opgevuld worden. Hierna wordt een optimizer opgezet voor de nieuwe pipeline met de development subset van de databank als referentie. Deze optimizer wordt gebruikt om de best mogelijke segmentatie drempel te vinden. De code hiervoor kan teruggevonden worden in codefragment \ref{code:eerste-finetuning}.

\begin{listing}
	\begin{minted}{python}
pipeline = SpeakerDiarization(
  segmentation=trained_model,
  clustering="OracleClustering"
)

pipeline.freeze({"segmentation": {"min_duration_off": 0.0}})

optimizer = Optimizer(pipeline)
dev_set = list(protocol.development())

iterations = optimizer.tune_iter(dev_set, show_progress=False)
best_loss = 1.0
for i, iteration in enumerate(iterations):
  print(f"Best segmentation threshold so far: {iteration['params']['segmentation']['threshold']}")
  if i > 50: break
  
best_segm_threshold = optimizer.best_params["segmentation"]["threshold"]
	\end{minted}
	\caption[Set-up voor eerste finetuning]{\label{code:eerste-finetuning}Eerste stap in het finetuning proces}
\end{listing}

Voor de tweede stap wordt de pipeline opgebouwd uit het getrainde segmentatie model, het embedding model, de embedding\_exclude\_overlap en de clustering uit de originele pipeline. Deze keer worden er parameters toegevoegd voor zowel de segmentatie, inclusief de beste waarde voor de drempel, als de clustering, waarna opnieuw een optimizer opgezet wordt aan de hand van de development subset uit de databank. Deze keer wordt de optimizer gebruikt voor het bepalen van een drempel voor de clustering. De code hiervoor kan gevonden worden in codefragment \ref{code:tweede-finetuning}.

\begin{listing}
	\begin{minted}{python}
pipeline = SpeakerDiarization(
  segmentation=trained_model,
  embedding=pretrained_pipeline.embedding,
  embedding_exclude_overlap=pretrained_pipeline.embedding_exclude_overlap,
  clustering=pretrained_pipeline.klustering,
)

pipeline.freeze({
  "segmentation": {
    "threshold": best_segm_threshold,
    "min_duration_off": 0.0
  },
  "clustering": {
    "method": "centroid",
    "min_cluster_size": 15
  }
})

optimizer = Optimizer(pipeline)
iterations = optimizer.tune_iter(dev_set, show_progress=False)
best_loss = 1.0
for i, iteration in enumerate(iterations):
  print(f"Best clustering threshold so far: {iteration['params']['clustering']['threshold']}")
  if i > 50: break
  
best_clust_threshold = optimizer.best_params["clustering"]["threshold"]
	\end{minted}
	\caption[Set-up voor tweede finetuning]{\label{code:tweede-finetuning}Tweede stap in het finetuning proces}
\end{listing}

Na de tweede finetuning stap wordt de pipeline opnieuw opgesteld zoals in de tweede stap van de finetuning. Deze pipeline wordt geïnstantieerd met dezelfde parameters als in de tweede stap en de beste drempelwaarde voor de clustering, zoals getoond wordt in codefragment \ref{code:finale-parameters}. Deze pipeline wordt dan geëvalueerd met pyannote-metrics om te controleren of de DER lager ligt dan in het oorspronkelijke model, analoog aan codefragment \ref{code:pyannote-metrics}. Hierna worden de parameters opgeslagen in een YAML bestand zodat de getrainde pipeline later opnieuw kan gebruikt worden zoals besproken wordt in hoofdstuk \ref{sec:gebruik}.

\begin{listing}
	\begin{minted}{python}
finetuned_pipeline.instantiate({
  "segmentation": {
    "threshold": best_segm_threshold,
    "min_duration_off": 0.0
  },
  "clustering": {
    "method": "centroid",
    "min_cluster_size": 15,
    "threshold": best_clust_threshold
  }
})
	\end{minted}
	\caption[Finale parameters]{\label{code:finale-parameters}Finale parameters waarmee de pipeline geïnstantieerd wordt na het trainen en finetunen.}
\end{listing}

\subsection{Resultaten van het trainen}
\label{subsec:train-res}

De dataset voor het trainen was in het begin opgebouwd met duidelijke audio van televisie programma's. Het trainen op deze data gaf veelbelovende resultaten. De Diarization Error Rate (DER) voor deze data was voor het trainen gelijk aan 9.2\% en door het trainen met deze audio kon de DER verlaagd worden naar 7.4\%. Als extra test werd hierna de pipeline toegepast op een audio van een Vlaams televisieprogramma en een audio bestand dat opgenomen werd in het Zorglab. Hier bleek dat de stemmen in de audio van het televisieprogramma vrij accuraat gescheiden konden worden, maar dat dit helemaal niet het geval was voor de audio uit het Zorglab.

Hierdoor werd besloten om de databank aan te passen. Er werd data uit het Zorglab toegevoegd aan alle subsets in de databank en het trainingsproces werd opnieuw uitgevoerd. Hierbij was de DER van de pipeline voor het trainen gelijk aan 20.9\%, wat significant hoger is dan bij de data uit de eerste versie van de databank. Na het trainen van het model op de nieuwe data was de DER hoger in plaats van lager, namelijk 56.4\%, wat een onverwachte uitkomst was. In een poging om dit resultaat te verbeteren, werd er extra data toegevoegd aan de subset voor het trainen en werd geëxperimenteerd met het aantal epochs in \lstinline{max_epochs} van het trainen. Het beste resultaat dat hierdoor bekomen werd was een DER van 50.6\% bij een waarde van 50 voor \lstinline{max_epochs}.

Aangezien het trainen met de nieuwe databank niet het gewenste resultaat gaf, werd er besloten om een ruisfilter toe te passen op de audiobestanden van het Zorglab en aan de hand van die bestanden het trainingsproces opnieuw uit te voeren. De ruisfilter die hiervoor gebruikt werd, is deze die door \textcite{Daems2023} reeds geïmplementeerd werd in de applicatie. Dit leverde voor het trainen een DER van 22.5\% op en na het trainen een DER van 52.3\%.

Hierdoor werd besloten om uiteindelijk toch verder te gaan met de eerste versie van de databank, mits een herverdeling van de data over de files, die dus leidde tot de derde versie van de databank die in sectie \ref{sec:data} beschreven is. De resultaten hiervoor waren een DER van 23.4\% voor het trainen en 13.9\% na het trainen.

\section{Gebruik maken van het getrainde model}\label{sec:gebruik}
Om de getrainde pipeline te gebruiken in een andere file, moet deze opnieuw opgebouwd worden met de het getrainde model en de parameters die opgeslagen werden na de finetuning. Dit gebeurt in twee stappen, aangezien de drempelwaardes die gevonden worden tijdens het finetunen geen parameters zijn in de oorspronkelijke pipeline. De eerste stap haalt de opgeslagen pipeline op en wordt geïnstantieerd met alle opgeslagen parameters behalve de drempelwaarden. Dit wordt getoond in codefragment \ref{code:eerste-stap}.

De tweede stap creëert een pipeline met de modellen uit de opgehaalde pipeline en wordt geïnstantieerd met alle opgeslagen parameters, inclusief de drempelwaarden. Dit wordt getoond in codefragment \ref{code:tweede-stap}.

\begin{listing}
	\begin{minted}{python}
trained_pipeline = Pipeline.from_pretrained(Path("./pipeline/config.yaml"))
model = Model.from_pretrained("./models/trained_model.ckpt")
trained_pipeline.segmentation_model = model

with open("./parameters/config.yaml", "r") as f:
  parameters = yaml.safe_load(f)

params = parameters["params"]

params_clust = params["clustering"]

trained_pipeline.instantiate({
  "segmentation": {"min_duration_off": params["segmentation"]["min_duration_off"]},
  "clustering": params["clustering"]
})
	\end{minted}
	\caption[Ophalen en instantiëren opgeslagen pipeline]{\label{code:eerste-stap}Ophalen van de opgeslagen pipeline en het instantiëren zonder de drempelwaarden}
\end{listing}

\begin{listing}
	\begin{minted}{python}
pipeline = SpeakerDiarization(
  segmentation=model,
  embedding=trained_pipeline.embedding,
  embedding_exclude_overlap=trained_pipeline.embedding_exclude_overlap,
  clustering=trained_pipeline.klustering,
)

with open("./parameters/config.yaml", "r") as f:
  loaded_parameters = yaml.safe_load(f)

loaded_params = loaded_parameters["params"]

params_clust = loaded_params["clustering"]

parameters_segm = parameters["params"]["segmentation"]


pipeline.instantiate({
  "segmentation": {
    "min_duration_off": parameters_segm["min_duration_off"],
    "threshold": parameters_segm["threshold"]
  },
  "clustering": params_clust
})
	\end{minted}
	\caption[Recreëren van de getrainde pipeline met alle parameters]{\label{code:tweede-stap}Recreëren en instantiëren van de pipeline na trainen en finetunen met alle parameters}
\end{listing}

Na deze stappen is de pipeline klaar voor gebruik. Om een audio bestand te splitsen, wordt het pad naar het bestand als argument meegegeven aan de pipeline. Hierna wordt een RTTM bestand gecreëerd aan de hand van de code uit codefragment \ref{code:maak-rttm}. Dit bestand wordt dan gebruikt om aparte audio bestanden te maken per instantie van een spreker in codefragment \ref{code:korte-bestanden}. Uiteindelijk worden deze audio fragmenten gecombineerd om audio bestanden te maken per spreker, zoals getoond in codefragment \ref{code:audio-per-spreker}.

\begin{listing}
	\begin{minted}{python}
file_name = re.search(r'.*\/(.+?)\.wav', path).group(1)

# delete file if it already exists so a new file can get created
if os.path.exists(f"./results/rttm/{file_name}.rttm"):
  os.remove(f"./results/rttm/{file_name}.rttm")

f = open(f"./results/rttm/{file_name}.rttm", "w")
for turn, _, speaker in diarization.itertracks(yield_label=True):
  f.write(f"SPEAKER {file_name} 1 {round(turn.start, 3)} {round(turn.end-turn.start, 3)} <NA> <NA> {speaker} <NA> <NA>\n")

f.close()
	\end{minted}
	\caption[Code om RTTM bestand te creëren]{\label{code:maak-rttm}Code die een RTTM bestand creëert uit het resultaat van de pipeline}
\end{listing}

\begin{listing}
	\begin{minted}{python}
audio = AudioSegment.from_wav(path)

os.mkdir(f"./results/audio/{file_name}/")


for turn, _, speaker in diarization.itertracks(yield_label=True):
  if not os.path.isdir(f"./results/audio/{file_name}/{speaker}"):
    os.mkdir(f"./results/audio/{file_name}/{speaker}")

  segment = audio[turn.start * 1000:turn.end * 1000]  # Convert seconds to milliseconds
  segment.export(f"./results/audio/{file_name}/{speaker}/speaker_{speaker}_{turn.start:.1f}-{turn.end:.1f}.wav", format="wav")
	\end{minted}
	\caption[Code om korte audio bestanden te creëren]{\label{code:korte-bestanden}Code die afzonderlijke audio fragmenten creëert gebaseerd op een RTTM bestand}
\end{listing}

\begin{listing}
	\begin{minted}{python}
speakers = os.listdir(f"./results/audio/{file_name}")

for speaker in speakers:
  files_list = os.listdir(f"./results/audio/{file_name}/{speaker}")
  files_list = natsorted(files_list)

  combined = AudioSegment.from_file(f"./results/audio/{file_name}/{speaker}/{files_list[0]}")
  for i in range(1,len(files_list)):
    combined = combined.append(AudioSegment.from_file(f"./results/audio/{file_name}/{speaker}/{files_list[i]}"), crossfade=0.0)

  combined.export(f"./results/audio/{file_name}_{speaker}.wav", format="wav")
	\end{minted}
	\caption[Code om audio bestanden per spreker te creëren]{\label{code:audio-per-spreker}Code die de afzonderlijke audio fragmenten combineert tot audio bestanden per spreker}
\end{listing}

\section{Implementatie in de applicatie}
\label{sec:implementatie}
Aangezien de resultaten van de pipeline na het trainen niet goed genoeg waren, werd het model nog niet geïmplementeerd in de applicatie.
